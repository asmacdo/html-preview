<!DOCTYPE html>
<head>
  <title>Projects</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="shortcut icon" href="/theme/img/logos/favicon.png" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300i-transtalic,400italic,600italic,700italic,300,400,600,700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,300,700,900,600,500,200" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/theme/vendor/bootstrap/dist/css/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="/theme/vendor/buddycloud-styles/dist/css/buddycloud-styles.css" type="text/css" />
  <link rel="stylesheet" href="/theme/css/site.css" type="text/css" />
 </head>
<body class="projects" style="overflow-y: scroll;">
  <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
      <div class="navbar-header" href="#">
          <button type="button" class="navbar-toggle" data-toggle="collapse"
              data-target="#navbar-collapse">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
          </button>
		  <div class="hidden-xs hidden-sm">
			<a class="navbar-brand" href="/">Center for Open Neuroscience</a>
		  </div>
		  <div class="hidden-md hidden-lg" style="font-size:12px">
			<a class="navbar-brand" href="/">Center for Open Neuroscience</a>
		  </div>

      </div>
      <div class="collapse navbar-collapse" id="navbar-collapse">
          <ul class="nav navbar-nav">
              <li class="active" >
                  <a href="/projects" >
                          Projects
                  </a>
              </li>
              <li  >
                  <a href="/whoweare" >
                          Who we are
                  </a>
              </li>
              <li  >
                  <a href="/engage" >
                          Engage
                  </a>
              </li>
              <li  >
                  <a href="/support" >
                          Support
                  </a>
              </li>
          </ul>
      </div>
  </div>
</div>
<div class="container bs-docs-container">
  <div class="row">
      <div class="col-md-3 col-sm-3">
    <div id="full-toc" class="bs-docs-sidebar hidden-print affix-top hidden-xs hidden-sm" role="complementary">
  <ul class='nav bs-docs-sidenav'><li class='active'><a href='#software_' data-scroll data-url>Software</a><ul class='nav'><li><a href='#datalad_' data-scroll data-url>DataLad</a></li><li><a href='#dandi_' data-scroll data-url>DANDI</a></li><li><a href='#duecredit_' data-scroll data-url>DueCredit</a></li><li><a href='#heudiconv___reproin_' data-scroll data-url>HeuDiConv / ReproIn</a></li><li><a href='#neurodebian_' data-scroll data-url>NeuroDebian</a></li><li><a href='#pymvpa_' data-scroll data-url>PyMVPA</a></li><li><a href='#reproman_' data-scroll data-url>ReproMan</a></li><li><a href='#con_tinuous_' data-scroll data-url>(con/)tinuous</a></li><li><a href='#pyout_' data-scroll data-url>pyout</a></li><li><a href='#quail_' data-scroll data-url>Quail</a></li><li><a href='#hypertools_' data-scroll data-url>HyperTools</a></li><li><a href='#supereeg_' data-scroll data-url>SuperEEG</a></ul></li><li><a href='#initiatives_' data-scroll data-url>Initiatives</a></li><ul class='nav'><li><a href='#open_brain_consent_' data-scroll data-url>Open Brain Consent</a></ul></li><li><a href='#standards_' data-scroll data-url>Standards</a></li><ul class='nav'><li><a href='#brain_imaging_data_structure_bids_' data-scroll data-url>Brain Imaging Data Structure (BIDS)</a></li><li><a href='#neurodata_without_borders__neurophysiology_nwb_n_' data-scroll data-url>Neurodata Without Borders: Neurophysiology (NWB:N)</a></ul></li><li><a href='#education_' data-scroll data-url>Education</a></li><ul class='nav'><li><a href='#repronim__reproducible_basics_' data-scroll data-url>ReproNim: Reproducible Basics</a></ul></li><li><a href='#infrastructure_' data-scroll data-url>Infrastructure</a></li><ul class='nav'><li><a href='#nibotmi_' data-scroll data-url>Nibotmi</a></li><li><a href='#singularityhub_afterlife_' data-scroll data-url>SingularityHub (after-life)</a></ul>
    </div>
      </div>
      <div class="col-md-9 col-sm-9" role="main" style="overflow-x:auto; padding-bottom: 10px;">
    <div class="bs-docs-section">
    
  <h1 style="margin-top:20px" id="software_">Software</h1>

  <div class="row">
	<h2 id="datalad_">DataLad</h2>
	<figure>
      <a href="https://datalad.org">
		<img src="/theme/img/3rd/datalad_logo.png" alt="DataLad" class="pull-right" style="width:255px;padding-left:5px;margin-top:-2em;"></a>
    </figure>
	<p>
	  <a href="https://datalad.org">DataLad</a> is ongoing work funded
	  by NSF and German BMBF, to adapt the model of open-source
	  software (OSS) distributions to address the technical
	  limitations of today's data-sharing and provides a versatile
	  data management platform.  It uses software for data tracking and
	  deployment logistics specialized for large data
	  (<a class="reference external" href="http://git-annex.branchable.com">git-annex</a>) built
	  atop <a class="reference external" href="http://git-scm.com">Git</a>, the most capable distributed
	  version control system (dVCS) available today. DataLad provides
	  access to data available from various sources (e.g. lab or
	  consortium web-sites such as <a class="reference external" href="http://humanconnectome.org">humanconnectome.org</a>;
	  data sharing portals such as <a class="reference external" href="http://openneuro.org">openneuro.org</a>
	  and <a class="reference external" href="http://crcns.org">crcns.org</a>) through a single
	  interface. It enables students and scientists to operate on data
	  using familiar concepts, such as files and directories, while
	  transparently managing data access and authorization with underlying
	  hosting providers.
	</p>
	<ul>
	  <li>
		  <a href="/whoweare#michael_hanke_" class="centroid">M. Hanke</a>,
		  <a href="/whoweare#matteo_visconti_di_oleggio_castello_" class="centroid">M. Visconti di Oleggio Castello</a>,
		  <a href="/whoweare#kyle_meyer_" class="centroid">K. Meyer</a>,
		  <a href="/whoweare#benjamin_poldrack_" class="centroid">B. Poldrack</a>,
		  and
		  <a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
		  (2018). <a href="https://github.com/myyoda/poster/blob/master/ohbm2018.pdf">YODA: YODA's organigram on data
			analysis.</a> OHBM 2018, Singapore.
	  </li>
	  <li>
		<a href="https://github.com/openjournals/joss-reviews/issues/3262">JOSS
		paper</a> (under review) providing a succinct overview of
		DataLad
	  </li>
	  <li>
		<a href="http://handbook.datalad.org">DataLad Handbook</a>:
		everything you need to know about DataLad.
	  </li>
	  <li> <a href="https://datalad.org/about.html#funding">Funding support</a> </li>
	</ul>

  </div>

    <div class="row">
	<h2 id="dandi_">DANDI</h2>
	<figure>
      <a href="https://dandiarchive.org"><img src="/theme/img/3rd/dandi_logo.png" alt="DANDI" class="pull-left" style="width:155px;padding-right:5px;margin-top:0em;"></a>
    </figure>
	<p>
	  <a href="https://dandiarchive.org">Distributed Archives for
	  Neurophysiology Data Integration (DANDI)</a> is a platform for
	  publishing, sharing, and processing neurophysiology data funded
	  by the <a href="https://braininitiative.nih.gov/">BRAIN
	  Initiative</a>. The platform is now available for data upload
	  and distribution, and provides supplementary client tools to
	  assist with introspection and organization of data
	  following <a href="#nwb">NWB standard</a>.
	</p>
	<ul>
	  <li>
		<a href="https://projectreporter.nih.gov/project_info_description.cfm?aid=9795271">NIH (#1R24MH117295-01A1)</a>. PIs:
		<a href="https://mcgovern.mit.edu/profile/satrajit-ghosh/" class="centroid">S. Ghosh</a>
		and
 		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
	  </li>
	</ul>

  </div>

  <div class="row">
	<h2 id="duecredit_">DueCredit</h2>
	<figure>
      <a href="https://datalad.org"><img src="/theme/img/3rd/duecredit_logo.png" alt="DueCredit" class="pull-left" style="width:155px;padding-right:10px;margin-top:0em;"></a>
    </figure>
	<p>
	  <a href="https://github.com/duecredit/duecredit">DueCredit</a>
	  provides solution for the problem of inadequate citation and
	  referencing of scientific software and methods.  It provides a
	  simple framework (at the moment for Python only) to embed
	  publication or other references in the original code so they are
	  automatically collected and reported to the user at the
	  necessary level of reference detail, i.e. only references for
	  actually used functionality will be presented back if software
	  provides multiple citeable implementations.
	</p>
	<p>
	  As a side-effect, we hope that DueCredit also will reduce demand
	  in "prima-ballerina" projects, will encourage contributions to
	  existing open-source codebases, and as a result would solidify
	  scientific software ecosystem.
	</p>
	<ul>
	  <li>
		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
		and
		<a href="/whoweare#matteo_visconti_di_oleggio_castello_" class="centroid">M. Visconti di Oleggio Castello</a>
		(2016).
		<a href="http://haxbylab.dartmouth.edu/publications/HV+OHBM16.pdf">
		  DueCredit - automagically collect citations for software,
		  methods, and data you use.
		</a> OHBM 2016, Geneva, Switzerland
	  </li>
	</ul>
  </div>

  <div class="row">
	<h2 id="heudiconv___reproin_">HeuDiConv / ReproIn</h2>
	<p>
	  <a href="https://github.com/nipy/heudiconv">HeuDiConv</a> is a
	  flexible DICOM converter for organizing brain imaging data into
	  structured directory layouts. As a part of the larger, NIH
	  supported <a href="http://repronim.org">ReproNim</a> effort, we
	  are developing a HeuDiConv-based <a href="http://reproin.repronim.org">ReproIn</a>
      solution for turnkey
	  automatic conversion of all collected MR data to a collection of
	  the BIDS DataLad datasets. It includes a flexible BIDS-like
	  specification how to name scanning sequences in the scanner, and
	  a <a href="https://github.com/nipy/heudiconv/blob/master/heudiconv/heuristics/reproin.py">HeuDiConv
	  reproin.py heuristic</a> to automate layout and conversion of
	  the datasets.  This solution is deployed
	  at <a href="http://dbic.dartmouth.edu">DBIC (Dartmouth Brain
	  Imaging Center)</a>, it already facilitates reproducible
	  research, data sharing, and uploads to central archives such as
	  NDA.
	</p>
	<ul>
	  <li>
		<a href="/whoweare#matteo_visconti_di_oleggio_castello_" class="centroid">M. Visconti di Oleggio Castello</a>,
		James E. Dobson,
		Terry Sackett,
		Chandana Kodiweera,
		<a href="/whoweare#james_v_haxby_" class="centroid">J.V. Haxby</a>,
		M. Goncalves,
		S. Ghosh,
		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
		<a href="http://goo.gl/Z3soj7">ReproIn: automatic generation of shareable,
		  version-controlled BIDS datasets from MR scanners</a>,
		OHBM 2018, Singapore.
	  </li>
	</ul>
  </div>

  <div class="row">
	<h2 id="neurodebian_">NeuroDebian</h2>
    <figure>
      <a href="https://neuro.debian.net"><img src="/theme/img/3rd/neurodebian_logo.jpg" alt="neurodebian" class="pull-right" style="padding-left:10px;margin-top:-2em;"></a>
    </figure>
	<p>
	  <a href="https://neuro.debian.net">NeuroDebian</a> is a turnkey
	  research software platform for all aspects of the
	  neuroscientific research process. It takes the ideas of the
	  software hosting portals such
	  as <a href="http://nitrc.org">NITRC</a> on maximizing research
	  transparency and methods sharing, one step further, by providing
	  a comprehensive suite of readily usable and fully integrated
	  software with a robust testing and deployment
	  infrastructure. Consequently, it improves interoperability among
	  the tools and frees researchers from the burden of tedious
	  installation or upgrade procedures. That, in turn, positively
	  affects their availability for actual research activities, as
	  well as their motivation to test new analysis tools and stay
	  connected with the latest methodological developments in the
	  field.
	  </p>

	<ul>
	  <li>
	  <a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
	  &amp; <a href="/whoweare#michael_hanke_" class="centroid">M. Hanke</a> (2012).
	  <a href="https://www.frontiersin.org/articles/10.3389/fninf.2012.00022/full">
		Open is not enough. Let's take the next step: An integrated, community-driven computing platform for neuroscience.</a>
	  Frontiers in Neuroinformatics, 6:22.
	  <a href="http://haxbylab.dartmouth.edu/publications/HH12.pdf">[PDF]</a> DOI: 10.3389/fninf.2012.00022
	  </li>
	</ul>
  </div>

  <div class="row">
	<h2 id="pymvpa_">PyMVPA</h2>
    <figure>
      <a href="http://pymvpa.org"><img src="/theme/img/3rd/pymvpa_logo.png" alt="PyMVPA" class="pull-left" style="width:395px;padding-right:10px;margin-top:0em;"></a>
    </figure>
	<p>
	  <a href="http://www.pymvpa.org">PyMVPA</a> is a Python-based
	  framework for neural decoding using multivariate pattern
	  analysis.  It affords both volume- and surface-based analyses
	  using a wide variety of supervised and unsupervised machine
	  learning methods, representational similarity analyses,
	  searchlight analyses, hyperalignment of representational spaces,
	  and model-based decoding and encoding.  The software also can be
	  used for neural data other than fMRI, including analysis of MEG
	  and EEG data through spatio-temporo-frequency band searchlights
	  and cross-modal EEG to fMRI trans-fusion.  It also has been used
	  for analyses on data unrelated to neuroscience, demonstrating
	  its general utility.  PyMVPA also serves as a repository for
	  sample data sets (e.g., Haxby et al. 2001) that has found wide
	  applicability for education, development of new algorithms, or
	  new analyses and independent research reports.
	</p>

	<ul>
	  <li>
		<a href="/whoweare#michael_hanke_" class="centroid">M. Hanke</a>,
		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>, et al. (2009).
		<a href="http://link.springer.com/article/10.1007%2Fs12021-008-9041-y">PyMVPA: A Python toolbox for multivariate pattern analysis of fMRI data.</a>
		Neuroinformatics, 7, 37-53. DOI: 10.1007/s12021-008-9041-y
	  </li>
	</ul>
  </div>


  <div class="row">
	<h2 id="reproman_">ReproMan</h2>
	<figure>
      <a href="http://reproman.repronim.org">
		<img src="/theme/img/3rd/niceman_logo.png" alt="NICEMAN" class="pull-right" style="width:155px;padding-left:5px;margin-top:-2em;">
	  </a>
    </figure>

	<p>
	  <a href="https://github.com/repronim/niceman">ReproMan
	  (Reproducible computational environments Manager; formerly known
	  as REPROMAN)</a> is also a
	  part of the NIH
	  supported <a href="http://repronim.org">ReproNim</a> effort.  It
	  aims to facilitate reproducible computation via collection of
	  detailed information about origin of the used components (Debian
	  and/or Conda packages, VCS repositories, etc), so that
	  computational environments could be analyzed, and re-created.
	</p>
	<ul>
	  <li>
		M. Travers, R. Buccigrossi, C. Haselgrove,
		<a href="/whoweare#kyle_meyer_" class="centroid">K. Meyer</a>,
		and
		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
		<a href="http://goo.gl/CvaY1e">NICEMAN: NeuroImaging
		  Computational Environments Manager</a>, OHBM 2018, Singapore.
	  </li>
	</ul>
  </div>

  <div class="row">
	<h2 id="con_tinuous_">(con/)tinuous</h2>
	<p>
	  <a href="https://github.com/con/tinuous/">tinuous</a> is a
	  command for downloading build logs and (for GitHub only)
	  artifacts and release assets for a GitHub repository from GitHub
	  Actions, Travis-CI.com, and/or Appveyor. By downloading them
	  all, and optionally placing them
	  under <a href="#datalad">DataLad</a> control you can establish
	  the backup, distribution, and convenient harmonious access to
	  all those artifacts.
	</p>
  </div>

  <div class="row">
	<h2 id="pyout_">pyout</h2>
	<figure>
	  <a href="https://github.com/pyout/pyout">
		<img src="/theme/img/3rd/pyout_logo.png" alt="pyout" class="pull-right" style="width:155px;padding-left:5px;margin-top:-2em;">
	  </a>
	</figure>
	<p>
	  <a href="https://github.com/pyout/pyout/">pyout</a> is a Python
	  package that defines an interface for writing structured records
	  as a table in a terminal. It is being developed to replace
	  custom code for displaying tabular data in
	  in <a href="#dandi">DANDI client</a> and others.
	</p>
  </div>

  
  <div class="row">
	    <h2 id="quail_">Quail</h2>
	    <figure>
          <a href="Quail">
		          <img src="/theme/img/3rd/quail_logo.png" alt="Quail" class="pull-left" style="width:155px;margin-right:30px;margin-top:0.5em;">
	        </a>
      </figure>

	    <p>
	        <a href="https://cdl-quail.readthedocs.io/en/latest/">Quail</a>
          is a Python toolbox for analyzing data from free recall memory experiments.  Some key features include:</p>
			<ul>
              <li> A simple data structure for storing encoding and recall data</li>
              <li> A set of functions for analyzing data by computing standard memory performance metrics</li>
              <li> A simple API for customizing plot styles</li>
              <li> Support for "naturalistic" stimuli such as movies, texts, and speech data</li>
              <li> A set of powerful tools for importing data, automatically transcribing audio files (speech-to-text), and more</li>
			</ul>
			<ul>
	        <li>A.C. Heusser, P.C. Fitzpatrick, C.E. Field, K. Ziman, and
	        <a href="/whoweare#jeremy_rothman_manning_" class="centroid">J.R. Manning</a>
	        (2017).
			<a href="http://joss.theoj.org/papers/3fb5123eb2538e06f6a25ded0a088b73">Quail: A Python toolbox for analyzing and plotting free recall data</a>.  The Journal of Open Source Software, 2(18): 424.</li>
	    </ul>
  </div>

  <div class="row">
	    <h2 id="hypertools_">HyperTools</h2>
	    <figure>
          <a href="HyperTools">
		          <img src="/theme/img/3rd/hypertools_logo.png" alt="HyperTools" class="pull-right" style="width:155px;padding-left:5px;margin-top:-2em;">
	        </a>
      </figure>

	    <p>
	        <a href="https://hypertools.readthedocs.io/en/latest/">HyperTools</a>
          is a Python toolbox for gaining geometric insights into high dimensional data.  Features include:
	    </p>
			<ul>
              <li>Functions for plotting high-dimensional datasets in 2D and 3D</li>
              <li>Static and animated plots</li>
              <li>Simple API for customizing plot styles</li>
              <li>Set of powerful data manipulation tools including hyperalignment, <i>k</i>-means clustering, normalizing, and more</li>
              <li>Support of lists of Numpy arrays, Pandas dataframes, text, or (mixed) lists</li>
              <li>Applying topic models and other text and word embedding methods to text data</li>
          </ul>
	    <ul>
	      <li>A.C. Heusser, K. Ziman, L.L.W. Owen, and
	        <a href="/whoweare#jeremy_rothman_manning_" class="centroid">J.R. Manning</a>
			(2018).  <a href="http://www.jmlr.org/papers/volume18/17-434/17-434.pdf">HyperTools: a Python Toolbox for Gaining Geometric Insights into High-Dimensional Data</a>.  Journal of Machine Learning Research, 18: 1-6.</li>
	    </ul>
  </div>

  <div class="row">
	    <h2 id="supereeg_">SuperEEG</h2>
	    <figure>
          <a href="SuperEEG">
		          <img src="/theme/img/3rd/supereeg_logo.png" alt="SuperEEG" class="pull-left" style="width:155px;padding-right:10px;margin-top:0em;">
	        </a>
      </figure>

	    <p>
	        <a href="https://supereeg.readthedocs.io/en/latest/">SuperEEG</a>
          is a Python toolbox for inferring whole-brain activity from sparse
          ECoG recordings. The way the technique works is to leverage data from
          different patients' brains (who had electrodes implanted in different
          locations) to learn a "correlation model" that describes how activity
          patterns at different locations throughout the brain relate. Given
          this model, along with data from a sparse set of locations, we use
          Gaussian process regression to "fill in" what the patients' brains
          were "most probably" doing when those recordings were taken. Details
          on our approach may be found in <a href="https://www.biorxiv.org/content/early/2018/10/12/121020">this
          preprint</a>. You may also be
          interested in watching <a href="https://www.youtube.com/watch?v=t6snLszEneA&amp;feature=youtu.be&amp;t=35">this
          talk</a> or reading this <a href="https://community.sfn.org/t/supereeg-ecog-data-breaks-free-from-electrodes/8344">blog
          post</a> from a recent conference.
	    </p>
	    <ul>
	      <li>L.L.W. Owen, A.C. Heusser, and
			<a href="/whoweare#jeremy_rothman_manning_" class="centroid">J.R. Manning</a>
			(2018).  <a href="https://www.biorxiv.org/content/early/2018/10/12/121020">A
	            Gaussian process model of human electrocorticographic data</a>.  bioRxiv, 121020.</li>
	    </ul>
  </div>

  <h1 id="initiatives_">Initiatives</h1>
  <div class="row">
	<h2 id="open_brain_consent_">Open Brain Consent</h2>
	<p>
	  <a href="http://open-brain-consent.readthedocs.org">Open Brain
	  Consent</a> initiative aims to facilitate neuroimaging data
	  sharing by providing an "out of the box" solution addressing
	  aforementioned human subjects concerns and consisting of
	</p>
	  <ul>
		<li>widely acceptable consent form allowing deposition of
		  anonymized data to public data archives</li>
		<li>collection of tools/pipelines to help anonymization of
		  neuroimaging data making it ready for sharing</li>
	  </ul>

	<ul>
	  <li>
		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>,
		C.F. Gorgolewski, et al.
	  </li>
	</ul>
  </div>

  
  

  <h1 id="standards_">Standards</h1>

  <div class="row">
	<h2 id="brain_imaging_data_structure_bids_">Brain Imaging Data Structure (BIDS)</h2>
	<figure>
      <a href="http://bids.neuroimaging.io">
		<img src="/theme/img/3rd/bids_logo.png" alt="BIDS" class="pull-right" style="width:255px;padding-left:10px;margin-top:-2em;">
	  </a>
    </figure>
	<p>
	  <a href="http://bids.neuroimaging.io">BIDS</a> is a
	  project lead by a steering group elected by the BIDS community
    to provide a simple and intuitive way to organize and describe your
	  neuroimaging and behavioral data.
	</p>
	<ul>
	  <li>
		Gorgolewski, K. J., et many,
		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
		et many more (2016).
		<a href="http://www.nature.com/articles/sdata201644">The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments.</a>
		Scientific Data, 3. DOI: 10.1038/sdata.2016.44
	  </li>
	  <li>
		<a href="https://github.com/bids-standard/bids-examples">Example (test)
		  datasets</a>
	  </li>
	  <li>
		<a href="http://datasets.datalad.org/?dir=/openfmri">Historical OpenfMRI</a> and up-to-date
		<a href="http://datasets.datalad.org/?dir=/openneuro">OpenNeuro</a>
		Datasets in DataLad distribution.
	  </li>
	</ul>
  </div>

  <div class="row">
	<h2 id="neurodata_without_borders__neurophysiology_nwb_n_">Neurodata Without Borders: Neurophysiology (NWB:N)</h2>
	<figure>
      <a href="https://www.nwb.org/">
		<img src="/theme/img/3rd/nwb_logo.png" alt="NWB" class="pull-left" style="width:255px;padding-right:10px;margin-top:0em;">
	  </a>
    </figure>
	<p>
	  <a href="https://www.nwb.org/">NWB:N</a> is a data standard for
	  neurophysiology, providing neuroscientists with a common
	  standard to share, archive, use, and build analysis tools for
	  neurophysiology data. It is a standard supported
	  by <a href="#bids">BIDS</a> and the <a href="#dandi">DANDI
		archive</a>.
	</p>
  </div>

  <h1 id="education_">Education</h1>
  <div class="row">
	<h2 id="repronim__reproducible_basics_">ReproNim: Reproducible Basics</h2>
	<p>
	  <a href="http://www.reproducibleimaging.org/module-reproducible-basics">Reproducible
	  Basics</a> training module of the ReproNim training curriculum
	  presents daily core tools (shell, version control, etc) and
	  explains how you could make your research more reproducible
	  having gained improved knowledge of them.
	</p>
	<ul>
	  <li>
		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
		et al.
	  </li>
	</ul>
  </div>


  <h1 id="infrastructure_">Infrastructure</h1>
  <div class="row">
	<h2 id="nibotmi_">Nibotmi</h2>
	<p>
	  <a href="https://github.com/nipy/nibotmi">NIPY BuildBot Master
	  Instance</a> was initiated by Matthew Brett to provide
	  continuous integration testing for the NiPy project. It quickly
	  grew up
	  to <a href="http://nipy.bic.berkeley.edu/waterfall">cover up a
	  wide variety of associated projects</a>
	  (e.g., <a href="http://nipy.org/dipy">Dipy</a>,
	  <a href="http://nipy.org/nipype">Nipype</a>, and
	  our <a href="/projects#pymvpa_">PyMVPA</a>).  Although it is
	  just an ad-hoc setup, it provides many project developers
	  testing environments which they could not otherwise easily
	  obtain elsewhere (e.g. on Travis-CI) -- various releases of
	  different operational systems (OS X, Windows, GNU/Linux Debian),
	  and even different architectures (e.g., PowerPC and SPARC).
	  Such rich coverage provides a valuable resource to the
	  scientific community helping to identify
	  defects <strong>before</strong> shipping releases to users. We
	  collaborate and help to maintaining the setup and a park of a
	  test boxes (e.g., SPARC machines).
	</p>
	<ul>
	  <li>
		<a href="/whoweare#matthew_brett_" class="centroid">M. Brett</a>,
		<a href="/whoweare#yaroslav_o_halchenko_" class="centroid">Y.O. Halchenko</a>
		et al.
	  </li>
	</ul>

  </div>

  <div class="row">
	<h2 id="singularityhub_afterlife_">SingularityHub (after-life)</h2>
	<p>
	  To provide archival and uninterrupted access to over 9TBs of
	  singularity-hub.org Singularity containers, we have
	  established <a href="https://datasets.datalad.org/?dir=/shub">///shub DataLad dataset</a> and a service to serve all
	  the shub:// URLs.
	</p>
  </div>


    </div>
          <span class="pull-right">
              <a class="btn btn-default" role="button" href="javascript:window.scrollTo(0,0);">Back to Top</a>
          </span>
      </div>
  </div>
</div>
<div class="index">
<div class="background-contact">
  <div class="container" id="contact">
    <div class="row">
      <div class="col-md-8">
        <h3>Center for Open Neuroscience</h3>
        <!-- <em>"Frameworks, software, data and methodologies for open neuroscience. Stay open! Become efficient! Produce reproducible!"</em> -->
		<em>Together we can make neuroscience a better science!</em>
      </div>
      <div class="col-md-3">
        <h3>Stay in touch</h3>
        <div class="contact">
		  <p class="address">Psychological and Brain Sciences, <br/>3 Maynard Street, Hanover, NH 03755, USA.</p>
		  <a href="mailto:team@centerforopenneuroscience.org" class="email">E-Mail</a>
		  <a href="https://twitter.com/centeropenneuro" class="twitter">Twitter</a>
		  <!-- <a href="http://blog.centerforopenneuroscience.org" class="blog">Blog</a>-->
		  <a href="https://github.com/con" class="github">Github</a>
          </br>
        </div>
        </br>
      </div>
    </div>
  </div>
</div>
</div>
<script src="/theme/vendor/jquery/dist/jquery.min.js"></script>
  <script src="/theme/vendor/bootstrap/dist/js/bootstrap.min.js"></script>
  <script src="/theme/vendor/buddycloud-styles/dist/js/buddycloud-styles.js"></script>
  <script src="/theme/vendor/underscore/underscore-min.js"></script>
  <script src="/theme/js/jquery.visible.min.js"></script>
<script type="text/javascript">
  $(window).scroll(function() {
    if ( $(".background-contact").visible(true) ){
      $("#full-toc").fadeOut(200, "linear");
      $("#habla_panel_div").fadeOut(200, "linear");
    }
    else{
      $("#full-toc").fadeIn(200, "linear");
      $("#habla_panel_div").fadeIn(200, "linear");
    }
  });

  function scroll_if_anchor(href) {
    href = typeof(href) == "string" ? href : $(this).attr("href");
    if (!href) return;
    var fromTop = 65;
    var $target = $(href);
    if ( $target.length ) {
      window.scrollTo(0, $target.offset().top - fromTop);
      if ( history && "pushState" in history ){
        history.pushState({}, document.title, window.location.pathname + href);
        return false;
      }
    }
  }
  $("#fixed-toc").css("visibility", "hidden");
  $(document).ready(function() {
    window.setTimeout(function() {
        scroll_if_anchor(window.location.hash);
    }, 1);
    window.setTimeout(function() {
        $("#fixed-toc").addClass("bs-docs-sidebar");
        $("#fixed-toc .nav").addClass("bs-docs-sidenav");
        $("#fixed-toc").css("visibility", "visible").hide();
        $("#fixed-toc").fadeIn(200, "linear");
    }, 200);
  });
</script>
<div class="website-licenses">
  Center For Open Neuroscience is not directly affiliated with <a href="http://centerforopenscience.org">COS (Center For Open Science)</a>.
  <br>
  Website content is copyright of respective authors and released under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a> license.  Website design is based on <a href="https://github.com/buddycloud/buddycloud.com">buddycloud.com</a>, released under Apache 2.0 license.
</div>
</body>
</html>